{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colorization\n",
    "\n",
    "**Fully Convolutional Networks: UNet Version**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from model import unet\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load UNet Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(param_path):\n",
    "    \n",
    "    net = unet.UNet()\n",
    "    net.load_state_dict(torch.load(param_path))\n",
    "    \n",
    "    return net\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_toPIL = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "])\n",
    "\n",
    "transform_totensor = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(image, net):\n",
    "    gen_images = net(image)\n",
    "    return gen_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(paths):\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = cv.imread(path)\n",
    "        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "        img = cv.resize(img, (32, 32))\n",
    "        #img = Image.open(path)\n",
    "        images.append(img)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gray(image):\n",
    "    r = image[0]\n",
    "    g = image[1]\n",
    "    b = image[2]\n",
    "    tensor = 0.299 * r + 0.587 * g + 0.114 * b\n",
    "    return tensor\n",
    "\n",
    "def batch_gray(images):\n",
    "    \"\"\"\n",
    "    Grayscale for Batch Images\n",
    "    :param images: [B, C, H, W]\n",
    "    :return: [B, H, W]\n",
    "    \"\"\"\n",
    "    batch_size = images.size()[0]\n",
    "    h = images.size()[2]\n",
    "    w = images.size()[3]\n",
    "    result = Variable(torch.zeros([batch_size, h, w]))\n",
    "    for i in xrange(batch_size):\n",
    "        result[i] = gray(images[i])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(gen_images, origin_images):\n",
    "    nums = 4\n",
    "    plt.figure(figsize=(100,80))\n",
    "    while i < nums*2:\n",
    "        plt.subplot(nums*2, 1, i)\n",
    "        i = i + 1\n",
    "        plt.imshow(gen_images[i])\n",
    "        plt.subplot(nums*2, 1, 1)\n",
    "        plt.imshow(origin_images[i])\n",
    "        i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'test_images/'\n",
    "image_paths = [image_dir+'%s.jpg' % x for x in range(1,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_images/1.jpg',\n",
       " 'test_images/2.jpg',\n",
       " 'test_images/3.jpg',\n",
       " 'test_images/4.jpg']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param = 'model_params/epoch_19_cpu_params.model'\n",
    "net = load_model(model_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_images(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_images = []\n",
    "for image in images:\n",
    "    transform_images.append(transform_totensor(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_one = transform_images[2]\n",
    "test_one.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_one = gray(test_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_one.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAADGklEQVR4nCXNy2sdZRgG8Od5v3dm\nzpmcnCTGNClN7cVUi9QbBZFStLoSK1hx60YX3brVjX+BiNCtuJCKKO5ceFmICwtCm6iIxWKtvcQo\nudU0yZzJzHzf48LfP/DjJABY90JdEykSuLnz5IFi+gLcnwKKm45kgCgRAIC8fqL4Qo0sPixCyRMB\noBOQCKr1g382U+2aA0MCFs0oCUY0UurSVlE2oZ+L48EBIjMlkCJSEIEkux6On5zg9NgQdBoN/99R\nSJLozl78ZYFVd4wUzCwBEghJAhXacOjWzGyY2c1oAmht20YBEQIltPG1fWdurox+CwECCAdSInvd\n6NrkPFQ8O+g/kD24CDwug8mCAYD0fCja9Z+vzi0E3Y3x+CuAkZDlMiQAOYjgfHmjWfz8L+8D5SwI\nYzA3ABpTiimmU9uPbt4+v9AfXNObe5eXbo0EyQFMbPc7dHHvcL3z2JHmszd6f5yt636zsiwaWSqL\nOY/G0T+ndx8ZpYVyfOnv88v/3pj7QCnCopPJLW4pK6sTt2N56fDM5MUXtfpVlbosd7TGktZBrGa5\ndq++cfDS9r250Z1qPTJL9c525fdtWmexF3yrDF2lpXp5dfrbo1VsBZIWfMOlyCRi606nbmBX9s/8\nhFITbZ0SI90ToKLJYfU2ilTfC3Ft/PoEcveQ2LQOSZkHSEzqRmMz3va0sQBmBtHF8QgrwqTZHgbZ\nTu6xijtlWpcOhShQHoEA7XnETBXzripU9wbbAjohwWBIHiR0wMmze5WyzGen7n9uCNxtmpja5MoI\npOiIw6l3Nt7t8dzExli2Zai66UjIPQNgUoLK0fyFzfm2+foAKKDZnEo0ywiCFA/1bSDu/lBkB/f/\n+jZEdpuWyQiSFBDXjbDe6TTx9LifBiBEBloIRgZQ9XIP4eL3zcflZHFOA4iYSh7MGIIRskl9iL6n\nT/hl8VHhn7IsQ8xaGYN5YELq2773rpyYezW7eun1gj+yzYcZCPdAKiXr5md/P+Z2uDizm7dxd+Wl\nU28trj30zWUe8SQAxd4zs2Or/WEc5gnz4bvsfdBhc/gP0P+ZxCKs/zgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32 at 0x10DE55910>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_one = gray_one.unsqueeze(0)\n",
    "transform_toPIL(gray_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_one = gray_one.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 32])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_one.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_input = Variable(gray_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = inference(gray_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAMK0lEQVR4nAEgDN/zAQtSJ3samU5R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\nhURPjTKVjZ5XBJ1yU6nwGS2xotT/7oSMWAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x11188C790>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_toPIL(p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAIqElEQVR4nE1WW4yeVRVde5/93f7r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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x10DC94250>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_toPIL(test_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_layer = net.GrayLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_one.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = gray_layer(test_one.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "pic should be Tensor or ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-421116758473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransform_toPIL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Python/2.7/site-packages/torchvision/transforms.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/torchvision/transforms.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mnpimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pic should be Tensor or ndarray'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mnpimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnpimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: pic should be Tensor or ndarray"
     ]
    }
   ],
   "source": [
    "transform_toPIL(mm.daa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
